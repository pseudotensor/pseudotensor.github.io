<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3.1"/>
<title>HARM: docs/optimizations.txt File Reference</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">HARM
   </div>
   <div id="projectbrief">harm and utilities</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="../../index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="../../annotated.html"><span>Data&#160;Structures</span></a></li>
      <li class="current"><a href="../../files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="../../search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="../../files.html"><span>File&#160;List</span></a></li>
      <li><a href="../../globals.html"><span>Globals</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">docs/optimizations.txt File Reference</div>  </div>
</div><!--header-->
<div class="contents">
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><div class="fragment"><div class="line"></div>
<div class="line"></div>
<div class="line">Performance Notes</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">0) Consider file writing and other bottlenecks</div>
<div class="line"></div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#a6fc838c14c5e9fc17d6d8bb3d868ec2f">PRODUCTION</a>==0 vs. 1 changes performance by about 20% with debugfail==2 <span class="keyword">set</span></div>
<div class="line"><a class="code" href="../../d8/d60/init_8c.html#a07484107e6d9fdf38b53edf631d6511d">E</a>.g.: Type of problem can change performance.  For example, same 64x32 model with <a class="code" href="../../d8/d5b/defs_8general_8h.html#aa4c9c8191d78fb683d1b8947f026afcb">R0</a>=0 <a class="code" href="../../d8/d5b/defs_8general_8h.html#a2b3e48ba92bae56a9925460125e78367">Rout</a>=40 gets 70<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> ZCPS <span class="keywordflow">while</span> <a class="code" href="../../d8/d5b/defs_8general_8h.html#aa4c9c8191d78fb683d1b8947f026afcb">R0</a>=-8.3 and <a class="code" href="../../d8/d5b/defs_8general_8h.html#a2b3e48ba92bae56a9925460125e78367">Rout</a>=1E10 gets 62<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> ZCPS.  Both have grid sectioning.  The change in performance here occurs because one has more failures than the other.  Set <a class="code" href="../../da/d3e/definit_8h.html#a6fc838c14c5e9fc17d6d8bb3d868ec2f">PRODUCTION</a> 0 allows no file writing of those failures and gets that 62<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> ZCPS run back to 70<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>.</div>
<div class="line"></div>
<div class="line">DODIAGS==0 or 1 can change things alot due to per-substep diagnostics being enabled.</div>
<div class="line">E.g.: File writing in general should be stretched in period to avoid slowing down code.  File writing can severely slow things down <span class="keywordflow">if</span> too frequent.  Consider ROMIO and Jon<span class="stringliteral">&#39;s non-blocking file writing.</span></div>
<div class="line"><span class="stringliteral">E.g.: Same model described above w.r.t. PRODUCTION=0/1 goes to 80K ZCPS with DODIAGS=0.  Ensure that ENERDUMPTYPE (and image/etc. creation) does not have too small a period.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">1) Consider cache use.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">For example, on 4 core system (Intel Core2), found that 512x32 model runs at 62K ZCPS while otherwise similar 64x32 model runs at 64K ZCPS.  As below states, could fit about 100 zones of data into cache, and having N1=64 (or N1M=72) allows entire line of data to fit into cache.  This reduces cache misses.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">On the other hand, if doing grid sectioning with large number of radial zones (e.g. 1024), then optimal to have entire N1M line on one node.  This will necessarily cause additional cache misses, but otherwise nodes won&#39;</span><a class="code" href="../../d8/d5b/defs_8general_8h.html#ae1ee1e1c83c3f70e2e1afed4f4f5b41e">t</a> be used and the performance drop is factors of 2<a class="code" href="../../dc/dd2/newcodediff_8txt.html#a933233ff9879962bcc2ff987c6920f90">X</a> or more instead of 5% as above.</div>
<div class="line"></div>
<div class="line">2) Consider each system<span class="stringliteral">&#39;s cache.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">e.g. TACC Ranger uses 4 sockets of Barcelona CPU:</span></div>
<div class="line"><span class="stringliteral">http://images.anandtech.com/reviews/cpu/amd/phenom2/barcelona-block-diagram.jpg</span></div>
<div class="line"><span class="stringliteral">Each core has 64KB private L1 cache (32K L1I and 32K L1D) (I=insruction D=data)</span></div>
<div class="line"><span class="stringliteral">Each core has *private* 512KB L2 cache.  This is nice compared to the shared L2 cache of my ki-rh42, even if that cache is 2X larger.</span></div>
<div class="line"><span class="stringliteral">Each Socket of 4 cores has *shared* 2MB L3 cache.</span></div>
<div class="line"><span class="stringliteral">Each node has 4 sockets each with 4 cores = 16 cores/node</span></div>
<div class="line"><span class="stringliteral">Node has 16 cores with 32GB/node</span></div>
<div class="line"><span class="stringliteral">The memory bus runs at 533Mhz with 2 channels total for *all* 16 cores!</span></div>
<div class="line"><span class="stringliteral">So staying in cache is *critical* since otherwise all 16 cores compete for 2 channels of memory.</span></div>
<div class="line"><span class="stringliteral">If using OpenMP, then one avoids otherwise MPI-excessive boundary cells, so good. [So this really makes OpenMP save on memory!!! Very important!]</span></div>
<div class="line"><span class="stringliteral">Can&#39;</span><a class="code" href="../../d8/d5b/defs_8general_8h.html#ae1ee1e1c83c3f70e2e1afed4f4f5b41e">t</a> fit entire problem in cache.  In reality, need to know how much memory use over longer periods of time.  Perhaps think per-line.  Then only need to ensure that can <span class="keywordflow">do</span> a single line.  Then can <span class="keywordflow">do</span> <a class="code" href="../../dc/dd2/newcodediff_8txt.html#a1df03026db3aa663afda0c9eed67408e">up</a> to about <a class="code" href="../../da/d3e/definit_8h.html#abd7b39be02bc15d79b73e5cf2b531299" title="#define CORNGDETVERSION 0">N1</a>=160 total on those 16 cores before cache-misses will occur.</div>
<div class="line"></div>
<div class="line">TACC Lonestar has 4<a class="code" href="../../d4/d96/global_8nondepmnemonics_8rad_8h.html#aa6b38d492364d98453284934ed7caee9">MB</a> L2 cache with 2.66Ghz Intel Xeon 5150.  TACC page calls it <span class="stringliteral">&quot;smart&quot;</span> L2 cache.</div>
<div class="line">Appears to have <span class="keyword">private</span> 2<a class="code" href="../../d4/d96/global_8nondepmnemonics_8rad_8h.html#aa6b38d492364d98453284934ed7caee9">MB</a> per core.</div>
<div class="line">http:<span class="comment">//www-dr.cps.intel.com/products/processor/xeon5000/specifications.htm?iid=products_xeon5000+tab_specs</span></div>
<div class="line">http:<span class="comment">//services.tacc.utexas.edu/index.php/lonestar-user-guide</span></div>
<div class="line">http:<span class="comment">//processorfinder.intel.com/List.aspx?ProcFam=528&amp;sSpec=&amp;OrdCode=</span></div>
<div class="line">http:<span class="comment">//processorfinder.intel.com/details.aspx?sSpec=SL9RU</span></div>
<div class="line">http:<span class="comment">//www.linuxdevices.com/files/misc/intel_5100.jpg</span></div>
<div class="line">http:<span class="comment">//www.hardwarezone.com/img/data/articles/2006/2002/Bensley-block-diagram-2.jpg</span></div>
<div class="line"></div>
<div class="line">Teragrid LONI QueenBee (Queen Bee):</div>
<div class="line">http:<span class="comment">//www.loni.org/systems/</span></div>
<div class="line">http:<span class="comment">//www.loni.org/teragrid/users_guide.php</span></div>
<div class="line">1) Connect to (e.g.) abe: ssh jmckinne@abe.ncsa.uiuc.edu</div>
<div class="line">2) Setup proxy: myproxy-logon -l jmckinne  -s myproxy.teragrid.org</div>
<div class="line">3) Use <a class="code" href="../../d3/d67/global_8general_8h.html#a61dfed37d81e56caf0e8554591cb599d">NCSA</a> Portal Password</div>
<div class="line">4) Connect to QB: gsissh login1-qb.loni-lsu.teragrid.org</div>
<div class="line"></div>
<div class="line">QUEENBEE: Must set <a class="code" href="../../d1/d5f/mytime_8h.html#a112c6e2175a2006eb3643eeac022c263">GETTIMEOFDAYPROBLEM</a> 1</div>
<div class="line"></div>
<div class="line">Teragrid <a class="code" href="../../d3/d67/global_8general_8h.html#a61dfed37d81e56caf0e8554591cb599d">NCSA</a> Abe:</div>
<div class="line">http:<span class="comment">//www.teragrid.org/userinfo/hardware/resources.php?select=single&amp;id=50&amp;PHPSESSID=0</span></div>
<div class="line">http:<span class="comment">//www.dell.com/content/products/productdetails.aspx/pedge_1955?c=us&amp;l=en&amp;s=corp</span></div>
<div class="line">5300 quad-core Xeon 5000 series</div>
<div class="line">2*4<a class="code" href="../../d4/d96/global_8nondepmnemonics_8rad_8h.html#aa6b38d492364d98453284934ed7caee9">MB</a> L2 cache (shared like ki-rh42).</div>
<div class="line"></div>
<div class="line">tgusage -u jmckinne</div>
<div class="line">tgusage -a TG-AST080026N</div>
<div class="line">tgusage -a TG-AST080025N</div>
<div class="line"></div>
<div class="line">Intel Core2 has same(?) as Barcelona but no L3 cache.</div>
<div class="line">JCM<span class="stringliteral">&#39;s system is: Intel(R) Core(TM)2 Extreme CPU Q6800  @ 2.93GHz stepping 0b</span></div>
<div class="line"><span class="stringliteral">Appears to have 2x4MB=8MB (shared) L2 cache and 32K/32K D/I L1 caches</span></div>
<div class="line"><span class="stringliteral">This 8MB of L2 cache is (however) shared so that 2 cores share 4MB and the other 2 shared 4MB.  So unless tune which core gets which process, each use of a new core slows down the L2 cache bandwidth by roughly 2X.  May be consistent with performance results below.</span></div>
<div class="line"><span class="stringliteral">So 2X cache compared to Ranger.</span></div>
<div class="line"><span class="stringliteral">http://www.intel.com/design/processor/datashts/316852.htm</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Intel Core i7:</span></div>
<div class="line"><span class="stringliteral">http://en.wikipedia.org/wiki/Intel_Core_3</span></div>
<div class="line"><span class="stringliteral">Note it has 32KB L1 D cache per core</span></div>
<div class="line"><span class="stringliteral">256KB L2 cache (I+D) per core</span></div>
<div class="line"><span class="stringliteral">8MB L3 cache (I+D) shared between *all* cores.</span></div>
<div class="line"><span class="stringliteral">Tri-channel memory</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Testing memory contention and whether sit entirely within L2 cache on JCM&#39;</span>s system without dual-channel access currently (so strong test of whether run fits in L2 cache or not):</div>
<div class="line"></div>
<div class="line">Multiple runs (performance for each core):</div>
<div class="line">#Cores:        1     2     3       4</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==2 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#aecdd356ed1a28e3325b6608f353cdc8f" title="ordered from most diffusive to least diffusive, so can start high and go down if needed 0 should be r...">DONOR</a>    64x32:   60<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>                 27<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>-30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a></div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 64x32:   50<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   45<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>           24<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>-27<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a></div>
<div class="line">MAXNBD==2 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#aecdd356ed1a28e3325b6608f353cdc8f" title="ordered from most diffusive to least diffusive, so can start high and go down if needed 0 should be r...">DONOR</a>    4x2:     30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>     30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [so finally fit entirely within L2 cache]</div>
<div class="line">MAXNBD==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 4x2:     18<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   18<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   18<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>     18<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [so finally fit entirely within L2 cache]</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 4x8:     30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   29.5<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>  29.3<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>  29<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [so finally fit entirely within L2 cache]</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 16x8:    50<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   48<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    48<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    48<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [so kinda hits memory]</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 32x16:   56<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   55<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    36<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    31<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [so 32x16*(2cores) is ok in cache, but not more (i.e. not this run with 32*16*3 or 32*16*4)]</div>
<div class="line">This suggests an optimal OpenMP choice is N1*<a class="code" href="../../da/d3e/definit_8h.html#acd864640121c7df2c19f61f7baa507e4">N2</a>*<a class="code" href="../../da/d3e/definit_8h.html#ad66109fbdc4d3adc9c3d6ff917038aef">N3</a>=32*32 per *node* to avoid memory contention and stay within L2 cache.  For elongated cells this is 512x2 in 2D or 256x2x2 in 3D.  But small cells leads to inefficiency in extra computations near boundary cells.  So probably want 256x4 in 2D and 64x4x4 in 3D.  On Ranger have to take (1/2) of this since L2+L3 is half the size!  So on Ranger use 128x4 in 2D and 32x4x4 in 3D with fit into L2+L3 cache.</div>
<div class="line"></div>
<div class="line">For above, probably good 4 core performance because staying in *L1* cache mostly, not L2.  This is consistent with below results.</div>
<div class="line">Then hit is because ki-rh42<span class="stringliteral">&#39;s 4 cores feed off 2 L2 cache&#39;</span>s so that bandwidth is half when 3-4 cores are running.</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">New code with even more optimizations (esp. fluxct and no eomfunc[NPR] data and now also reduction of symmetric matrices):</div>
<div class="line"></div>
<div class="line">Multiple runs (performance for each core):</div>
<div class="line">#Cores:        1     2     3       4</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 16x8:    51<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   51<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    51<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    50<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [4 core perf not helped with symm. matrix reductions]</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 32x16:   60<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   60<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    54<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    50<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [so cache misses not as bad with new code. Reducing symmetric matrices increased performance at 4 cores by 25%, so good.]</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 64x16:   60<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   59<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    48<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>    36<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [sym matrix fix increased perf for 4 cores by 25%]</div>
<div class="line"></div>
<div class="line">OpenMP (i.e. <a class="code" href="../../d3/d67/global_8general_8h.html#a126b8328170a52811a990e5dd302f783">USEOPENMP</a>==1 in makehead.inc) tests:</div>
<div class="line"></div>
<div class="line">#Cores:        1     2     3       4</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 16x8:    37<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   48<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   58<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>     66<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>       [horrendous 4 core performance -- 10% improvement with minchunk of 10 or static schedule compared to guided.  Didn<span class="stringliteral">&#39;t help with 1 core overhead.  Reducing memory overhead didn&#39;</span><a class="code" href="../../d8/d5b/defs_8general_8h.html#ae1ee1e1c83c3f70e2e1afed4f4f5b41e">t</a> help, suggesting it<span class="stringliteral">&#39;s all OpenMP overhead!]</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 64x16:   48K   78K   88K     99K       [Unsure what&#39;</span>s memory vs. OpenMP overhead, but big hit compared to above runs!  Reducing memory overhead didn<span class="stringliteral">&#39;t help at all for 4 cores!  Suggests all OpenMP overhead!  Removed as much of private() [all essentialy] and copyin() [as much as I could] and little change! Even ALLOWKAZEOS is off! -- Must be EOS ptr functions?  Just stupid addresses!]</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 32x16:   45K   69K   83K     92K</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Even with memory contention, 4 cores here should get about 4*40K=160K or 4*30K=120K.  Appears to be 20%-25% overhead per core no matter how many (even 1) cores.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Also appears to be extreme overhead no matter what when doing &lt;200 or so iterations.  Requires per node memory be (say) 64x16 or larger.  But L2 cache requires 64*16*2 or smaller.  So seems 64x16 - 64x32 (or 32x32) is sweet spot for minimizing OpenMP overhead and L2 cache misses.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">To test OpenMP overhead, I commented out all #pragma&#39;</span>s but left -openmp during compilation and checked 1 core:</div>
<div class="line"></div>
<div class="line">#Cores:        1     2     3       4</div>
<div class="line"><a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 64x16:   61<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a></div>
<div class="line"></div>
<div class="line">So not -openmp itself, but #pragma<span class="stringliteral">&#39;s cause the problem.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Now I tried only commenting out inner loop pragmas:</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">#Cores:        1     2     3       4</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 64x16:   46K</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">So not -openmp or #pragma omp for, but #pragma omp parallel is problem.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Now I tried commenting out loops and choosing OPENMPGLOBALPRIVATE (etc.) to be nothing (can do this since globals preserved on master):</span></div>
<div class="line"><span class="stringliteral">Also commented out #pragma omp threadprivate....</span></div>
<div class="line"><span class="stringliteral">And commented out parallel copyin( -&gt; //copyin(</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">#Cores:        1     2     3       4</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 64x16:   61K</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">So problem appears to be copyin() .... with same 64x16 test:</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">TEST1CORE: without copyin(EOS ptr&#39;</span>s) reach 52<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> for 1 core, still overhead?</div>
<div class="line">TEST1CORE: without copyin(EOS ptrs<span class="stringliteral">&#39; + all loop stuff except ploop): reach 52K : still overhead!</span></div>
<div class="line"><span class="stringliteral">TEST1CORE: without copyin(EOS ptrs&#39;</span> + all loop stuff): reach 57<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> : finally little overhead!</div>
<div class="line"></div>
<div class="line">NEWCODE1: with all required things as thread private and copyin() [had too many things for loops]: 51<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a></div>
<div class="line">NEWCODE2: with EOS ptrs totally fixed (outside parallel and not thread private anymore): 54<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>-55<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> for 1 core [good!] 102<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> or <a class="code" href="../../dc/dd2/newcodediff_8txt.html#a1df03026db3aa663afda0c9eed67408e">up</a> to 112<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> for 4 cores [still bad]</div>
<div class="line">NEWCODE3: with no {ijk}curr, whichuconcalc, etc. that required extensive code adjustments <span class="keywordflow">for</span> the EOS-related stuff to force modularity: 57<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> <span class="keywordflow">for</span> 1 core and 104<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> <span class="keywordflow">for</span> 4 cores : So quite little OpenMP overhead now <span class="keywordflow">for</span> 1 core, but still there <span class="keywordflow">for</span> 4 cores.</div>
<div class="line"></div>
<div class="line">So 1 core overhead is quite small.  Now unsure why 4 core bad since not memory limited since changing cores doesn<span class="stringliteral">&#39;t change problem size.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Installed all memory into ki-rh42 (8GB total) so uses dual-channel memory access:</span></div>
<div class="line"><span class="stringliteral">1) OpenMP 4 cores went from 86K -&gt; 107K for 512x32 testreality problem.</span></div>
<div class="line"><span class="stringliteral">2) MPI 4 cores (no grid sectioning so uses all cores) went from 90K -&gt; 110K</span></div>
<div class="line"><span class="stringliteral">So despite all Jon&#39;</span>s hard work with OpenMP, MPI still faster?  Can<span class="stringliteral">&#39;t be large overhead with OpenMP at 512x32.</span></div>
<div class="line"><span class="stringliteral">Overall, performance is nearly as expected for simulation that doesn&#39;</span><a class="code" href="../../d8/d5b/defs_8general_8h.html#ae1ee1e1c83c3f70e2e1afed4f4f5b41e">t</a> fit entirely into L2 cache that would be roughly 31<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>/core * 4 = 120<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>. </div>
<div class="line"></div>
<div class="line"><span class="preprocessor">#Cores:        1     2     3       4</span></div>
<div class="line"><span class="preprocessor"></span>MPI <a class="code" href="../../da/d3e/definit_8h.html#ac97bae4266eb010870a1da7dc312c459" title="#if(DOENOFLUX==ENOFINITEVOLUME)">MAXBND</a>==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 64x16:                       72<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> (64x16 total with 32x8 per core of 4 cores) -- only runs at 40% of core per process!</div>
<div class="line">OMP MAXBND==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a> 64x16:                       130<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> (only a bit over 2<a class="code" href="../../dc/dd2/newcodediff_8txt.html#a933233ff9879962bcc2ff987c6920f90">X</a> expected performance from 57<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>/core from NEWCODE3.  Not sure what<span class="stringliteral">&#39;s limiting since should be all in L2 cache!)</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Clearly for small problem sizes per core, OpenMP is much more efficient than MPI.  For large problem sizes per core (e.g. 256x16 per core for 512x32 total), MPI is just slightly more efficient.  Thus, in general one is benefited by using OpenMP.  Unclear how things scale with many more processores or cores.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Still problem is that efficiency from 1 core to 4 cores is 50% for either case, and again not because of L2 cache [Well, ki-rh42&#39;</span>s L2 cache is effectively shared, so may explain it.]</div>
<div class="line"></div>
<div class="line">Suggests that, <span class="keywordflow">for</span> Ranger at least, should <span class="keywordflow">try</span> to fit into <span class="keyword">private</span> 512KB L2 cache.  Can<span class="stringliteral">&#39;t use above tests to conclude what size this is for ki-rh42.  Can only run 2 cores and see when adding 2nd core slows down (assumes each core uses essentially its own nearby cache).  Once 2nd core slows things down significantly, then must have gone to memory.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">2CORE ki-rh42 test (latest NEWCOD3) for fitting into L2 cache:</span></div>
<div class="line"><span class="stringliteral">Multiple runs (performance for each core, OpenMP disabled):</span></div>
<div class="line"><span class="stringliteral">#Cores:        1     2    3     4</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE  64x16:   63K   63K</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE  64x32:   61K   60K</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE  64x64:   56K   58K</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 128x128:  61K   57K             [Starting to hit memory]</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 256x256:  60K   56K 50K    40K  [Starting to hit memory]</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">OpenMP:</span></div>
<div class="line"><span class="stringliteral">#Cores:        1     2    3     4</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE 256x256: 59K   95K  114K  116K  [Consistent with (roughly) 2 cores sharing 4MB of L2 cache, so with 3-4 cores goes much slower per core due to cache contention.]</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE  64x16:  59K   99K  122K  128K  [&quot;&quot; -- roughly]</span></div>
<div class="line"><span class="stringliteral">MAXBND==4 PARALINE  32x16:  59K   90K  115K  119K  [Linux probably doesn&#39;</span><a class="code" href="../../d8/d5b/defs_8general_8h.html#ae1ee1e1c83c3f70e2e1afed4f4f5b41e">t</a> schedule processes based upon L2 cache association with cores, so can flip around and almost as bad as all 4 cores sharing L2 cache.]</div>
<div class="line"></div>
<div class="line">Perf with STAG+DISS+DISSVSR+LUMVSR    = 30<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> -- so about 2<a class="code" href="../../dc/dd2/newcodediff_8txt.html#a933233ff9879962bcc2ff987c6920f90">X</a> slower.</div>
<div class="line">Perf with STAG+DISS+DISSVSR+LUMVSR+3D = 11<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a> -- so about 6<a class="code" href="../../dc/dd2/newcodediff_8txt.html#a933233ff9879962bcc2ff987c6920f90">X</a> slower.</div>
<div class="line"></div>
<div class="line">TODO:</div>
<div class="line">1) <span class="keywordflow">if</span> limiting interpolation (e.g. <span class="keywordflow">for</span> stag or rescale in stag), then pass that fact rather than <span class="keyword">using</span> global npr2interp.  Ensure all interior loops use that passed data rather than globals.</div>
<div class="line">2) For KAZ stuff, probably fine.</div>
<div class="line"></div>
<div class="line">Performance on Ranger (All MAXBND==4 <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a>) for 1000 steps with <a class="code" href="../../d8/d5b/defs_8general_8h.html#aac5486df6a846aa6cb47c73202ac0485">DODIAGS</a>=0 and <a class="code" href="../../da/d3e/definit_8h.html#a6fc838c14c5e9fc17d6d8bb3d868ec2f">PRODUCTION</a> 1 and <a class="code" href="../../d8/d5b/defs_8general_8h.html#ae202d8b69a0ef3f07651368c3de9ca25">TIMEORDER</a>=2 and <a class="code" href="../../d8/d5b/defs_8general_8h.html#a11c4e99e1493d0c1914ad1ab313ea472">FLUXB</a>=<a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a5014e44c6e710d8827ef79dd18abe6ad">FLUXCTTOTH</a> and <a class="code" href="../../da/d3e/definit_8h.html#a702c3f736ffae96949fb85f091aff230">DODISS</a>=<a class="code" href="../../da/d3e/definit_8h.html#affb54fad9620541e1fcb17f854af9bfe" title="see diag_source()">DODISSVSR</a>=<a class="code" href="../../da/d3e/definit_8h.html#a6c2751c5217b428c5f5faf66a08e2d90" title="0: don&#39;t do calculation 1: do">DOLUMVSR</a>=0:</div>
<div class="line"></div>
<div class="line">N1xN2    <span class="preprocessor">#NODES  #OPENMP/task  #MPItasks  ncpux?      PERF   Eff</span></div>
<div class="line"><span class="preprocessor"></span></div>
<div class="line">64x64:      1       1             1        1x1x1       35<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>  100%</div>
<div class="line">64x64:      1       4             4        2x2x1      403<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   72%</div>
<div class="line">64x64:      1       1            16        4x4x1      407<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   73%</div>
<div class="line">64x64:      1      16             1        1x1x1       71<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   13%</div>
<div class="line">64x64:      1       8             2        2x1x1      215<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   39%</div>
<div class="line">64x64:      2       1            32        8x4x1      940<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   84%</div>
<div class="line">64x64:      2       4             8        4x2x1      950<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   85%</div>
<div class="line">64x64:      2       1            64        8x8x1     1793<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   80%</div>
<div class="line">64x64:      2       4            16        4x4x1     1862<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   83%</div>
<div class="line">64x64:      2       1           256        16x16x1   7098<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   80%   </div>
<div class="line">64x64:      2       4           64         8x8x1     6766<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   76%</div>
<div class="line"></div>
<div class="line">64x16:      1       1             1        1x1x1       35<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a></div>
<div class="line">64x16:      1       4             4        2x2x1      406<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   72%</div>
<div class="line">64x16:      1       1            16        4x4x1      471<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   84%</div>
<div class="line">64x16:      1      16             1        1x1x1       55<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   10%</div>
<div class="line">64x16:      1       8             2        2x1x1      191<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   34%</div>
<div class="line">64x16:      2       1            32        8x4x1      907<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   81%</div>
<div class="line">64x16:      2       4             8        4x2x1      777<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   70% [repeated run got same result!?]</div>
<div class="line">64x16:      2       1            64        8x8x1     1730<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   77%</div>
<div class="line">64x16:      2       4            16        4x4x1     1488<a class="code" href="../../d8/d5b/defs_8general_8h.html#a33aba725064a95e4c33b25ccdc2504a9">K</a>   66%</div>
<div class="line"></div>
<div class="line">So each Ranger core is almost 2<a class="code" href="../../dc/dd2/newcodediff_8txt.html#a933233ff9879962bcc2ff987c6920f90">X</a> slower than ki-rh42.  That<span class="stringliteral">&#39;s AMD vs. Intel for you!</span></div>
<div class="line"><span class="stringliteral">So clearly bad to cross on PCI bus with memory as OpenMP has to when more than 4 threads with 1 thread per core.</span></div>
<div class="line"><span class="stringliteral">MPI seems to be doing fine at 64^2 on one node.</span></div>
<div class="line"><span class="stringliteral">Unclear why OpenMP is actually slower even for 64x16, which worked better on ki-rh42 by 2X!</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">Performance on Ranger (All MAXBND==4 PARALINE) for 1000 steps with DODIAGS=0 and PRODUCTION 1 and TIMEORDER=2 and FLUXB=FLUXCTSTAG and DODISS=DODISSVSR=DOLUMVSR=1:</span></div>
<div class="line"><span class="stringliteral">Default: module unload mvapich2 pgi ; module load mvapich2 intel mkl</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">N1xN2    #NODES  #OPENMP/task  #MPItasks  ncpux?      PERF   Eff</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">64x32x8:    1       1             1        1x1x1        6K</span></div>
<div class="line"><span class="stringliteral">64x32x32:   1       1             1        1x1x1        8.7K        [to be used as reference for efficiency for 64x32x32 per MPI task runs]</span></div>
<div class="line"><span class="stringliteral">64x32x8:    4       4             4        2x2x1       41K   43%</span></div>
<div class="line"><span class="stringliteral">64x32x8:   16       1            16        4x4x1       84K   88%</span></div>
<div class="line"><span class="stringliteral">64x32x8:   64       4            64        8x8x1      617K   40%</span></div>
<div class="line"><span class="stringliteral">64x32x8:   64       4            64        8x8x1      624K   41%  [used purely static schedule, no user chunking]</span></div>
<div class="line"><span class="stringliteral">128x64x8:  64       4            64        8x8x1      713K   46%  [used purely static schedule, no user chunking]</span></div>
<div class="line"><span class="stringliteral">64x32x32:  64       4            64        8x8x1      884K   40%-57%  [used purely static schedule, no user chunking] [smaller % is when using correct reference point]</span></div>
<div class="line"><span class="stringliteral">64x32x32:  64       4            64        8x8x1      896K   40%-58%  [Changed to mvapich/1.0.1 during compile and in batch script]</span></div>
<div class="line"><span class="stringliteral">64x32x32:  64       4            64        8x8x1      888K   39%-58%  [Changed to openmpi/1.3 during compile and in batch script]</span></div>
<div class="line"><span class="stringliteral">64x32x8:  256       1           256        8x8x4     1278K   83%</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">On ki-rh42 with new code:</span></div>
<div class="line"><span class="stringliteral">N1xN2    #NODES  #OPENMP/task  #MPItasks  ncpux?      PERF   Eff</span></div>
<div class="line"><span class="stringliteral">64x32x8:    1       1             1        1x1x1       17K</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">From 11K -&gt; 17K -- not that big a difference after a day of pain.</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">On Ranger: 4x4x4 per CPU with PARALINE+STAG+DISS+LUM (but R0=0 and Rout=10) on 2048 processors and 16x16x8 for ncpux?: late-time with 54K ZCPS with average of 10% fractional diagnostics</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">FULLSTAG=UNFUDDLE,STAG, RK2, DODISS, currents, entropy evolution, etc.etc.</span></div>
<div class="line"><span class="stringliteral">HALFSTAG=UNFUDDLE,STAG,RK2, no features</span></div>
<div class="line"><span class="stringliteral">HALFTOTH=UNFUDDLE,Toth,RK2, no features</span></div>
<div class="line"><span class="stringliteral">ORIG=Original very lean (and crashy-unstable) 2D HARM, which is on unfuddle as &quot;origcode&quot;</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">system   tile_size   cores   zcps   efff         CODE-MODE</span></div>
<div class="line"><span class="stringliteral">----------------------------------------------------------------------</span></div>
<div class="line"><span class="stringliteral">3D:</span></div>
<div class="line"><span class="stringliteral">lonestar  34x32x8    1        13K       1            FULLSTAG</span></div>
<div class="line"><span class="stringliteral">lonestar  34x32x8    1        14K       1            HALFSTAG</span></div>
<div class="line"><span class="stringliteral">lonestar  34x32x8    1        28K       1            HALFTOTH</span></div>
<div class="line"><span class="stringliteral">lonestar  34x32x8    1024     9.5M      71%          FULLSTAG</span></div>
<div class="line"><span class="stringliteral">lonestar  34x8x8     1        ?         1            FULLSTAG</span></div>
<div class="line"><span class="stringliteral">lonestar  34x8x8     512      3.3M      &gt;50%?        FULLSTAG</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">ki-rh42    32x16x8    1         14K      1           FULLSTAG</span></div>
<div class="line"><span class="stringliteral">ki-rh42    32x16x8    1         15K      1           HALFSTAG</span></div>
<div class="line"><span class="stringliteral">ki-rh42    32x16x8    1         28K      1           HALFTOTH</span></div>
<div class="line"><span class="stringliteral">ki-rh42    16x32x32   1         30K      1           HALFTOTH (Noble setup, who got 36K)</span></div>
<div class="line"><span class="stringliteral">2D:</span></div>
<div class="line"><span class="stringliteral">ki-rh42    64x64      1         57K       1          HALFTOTH</span></div>
<div class="line"><span class="stringliteral">ki-rh42    256x256    1         61K       1          HALFTOTH</span></div>
<div class="line"><span class="stringliteral">ki-rh42    64x64      1         80K       1          ORIG</span></div>
<div class="line"><span class="stringliteral">ki-rh42    256x256    1         84K       1          ORIG</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">http://www.intel.com/design/core2XE/documentation.htm</span></div>
<div class="line"><span class="stringliteral"></span></div>
<div class="line"><span class="stringliteral">1) Best if arrays are powers of 2 so compiler bit shifts to index array</span></div>
<div class="line"><span class="stringliteral">2) Try to keep all memory local.  Cache-lines are grabbed along fastest memory portions, so avoid loops that skip over indices.  Also best to compact multiple arrays together into a single per-point array or structure if those arrays are accessed together since then avoids cache miss when having to grab other array that will be displaced due to its full size.</span></div>
<div class="line"><span class="stringliteral">3) Try to reduce memory footprint (total arrays, global or not) (and also reduce N1,N2,N3) so code+data fit into L2 cache (~4MB+)</span></div>
<div class="line"><span class="stringliteral">4) Try to ensure code+data at function level can fit into L1 cache (32K)</span></div>
<div class="line"><span class="stringliteral">5) Esp. for multi-core apps, want to fit MUCH of data into L2 cache</span></div>
<div class="line"><span class="stringliteral">This corresponds to (currently) using 25K/cell including BZones</span></div>
<div class="line"><span class="stringliteral">So (e.g.) N1M=N2M=12 (i.e. N1=N2=4) would fit completely into L2 cache of 4MB</span></div>
<div class="line"><span class="stringliteral">In reality, just need to fit code+data along several cache lines to avoid memory being accessed.  Code is currently directed in memory along N3, then N2, then N1.  So optimal for N3&gt;&gt;N2&gt;&gt;N1.</span></div>
<div class="line"><span class="stringliteral">Should allow N1=phi, N2=theta, N3=r in case want many r cells per core</span></div>
<div class="line"><span class="stringliteral">Instead of changing [N1M][N2M][N3M], should change from [i][j][k] -&gt; [k][j][i] for example.  So macrofy this.</span></div>
<div class="line"><span class="stringliteral">Similar, but more involved than what Sasha does with his init&#39;</span>s.</div>
<div class="line"></div>
<div class="line">http:<span class="comment">//www.eventhelix.com/RealtimeMantra/Basics/OptimizingCAndCPPCode.htm</span></div>
<div class="line"></div>
<div class="line">When arrays of structures are involved, the compiler performs a multiply by the structure size to perform the array indexing. If the structure size is a power of 2, an expensive multiply operation will be replaced by an inexpensive shift operation. Thus keeping structure sizes aligned to a power of 2 will improve performance in array indexing.</div>
<div class="line"></div>
<div class="line">3) Note that for very small N1,<a class="code" href="../../da/d3e/definit_8h.html#acd864640121c7df2c19f61f7baa507e4">N2</a>,<a class="code" href="../../da/d3e/definit_8h.html#ad66109fbdc4d3adc9c3d6ff917038aef">N3</a>, ZCPS will appear to drop even if internal performance is the same.  This is because (e.g.) if N1=32 and N2=2, then in 2-direction it&#39;s really like doing N2=4 because of the extra 2 zones cmputed for the surface flux.  In reality not all parts of the code do this, so affected performance is between 1-2<a class="code" href="../../dc/dd2/newcodediff_8txt.html#a933233ff9879962bcc2ff987c6920f90">X</a> factor.</div>
<div class="line"></div>
<div class="line">Use pfmon (perfmon2) to look at cache and memory problems.</div>
<div class="line">Note that perfmon2 by default only looks at a single thread, but shows OpenMP usage for that thread so still useful.  Also shows pow() and other library calls that gprof does not.</div>
<div class="line">Note that perfmon2&#39;s HALT cycles may be larger for faster code in a repeated way (not just other procs eating time).  This doesn&#39;<a class="code" href="../../d8/d5b/defs_8general_8h.html#ae1ee1e1c83c3f70e2e1afed4f4f5b41e">t</a> mean if time without perfmon that will be slower.  For example, I found OpenMP&#39;ing <a class="code" href="../../d4/dd3/bounds_8tools_8c.html#a5bc91fa94289e7828d62831b8e3ff032" title="interpolate across pole to remove numerical errors there Note that this function is done over all zon...">poledeath</a>() led to more HALT cycles but clearly shorter wall time.</div>
<div class="line"></div>
<div class="line">See installperfstuff.txt and compilekernel.txt</div>
<div class="line"></div>
<div class="line">Use gprof:</div>
<div class="line">1) Compile with -g -pg</div>
<div class="line">2) Run test code</div>
<div class="line">3) Run: gprof ./grmhd &gt; prof.txt</div>
<div class="line">4) Run: gprof -l ./grmhd &gt; profbyline.txt</div>
<div class="line">5) Look at prof.txt top and bottom portions to get idea of what is bottleneck</div>
<div class="line">6) Use profbyline.txt to see if issue with bottleneck happens to be single lines that can be improved.</div>
<div class="line"></div>
<div class="line">In any case of pfmon or gprof, compile with -fno-inline (remove other inline commands) to see inlined functions expanded so can tell what interior functions take time within a function that was previously inlined.  Use __inline keyword in front of those functions one wants to see not inlined.</div>
<div class="line"></div>
<div class="line">Note that while gprof seems to miss some functions (e.g. pow() in icc library), pfmon catches it as pow.L .  </div>
<div class="line"></div>
<div class="line">One can use gprof-helper.c to have gprof measure multi-thread performance to see overhead.</div>
<div class="line">See inside of gprof-helper.c:</div>
<div class="line">do:</div>
<div class="line">1) gcc -shared -fPIC gprof-helper.c -o gprof-helper.so -lpthread -ldl</div>
<div class="line">2) (e.g.) LD_PRELOAD=./gprof-helper.so grmhd 4 1 1 1</div>
<div class="line"></div>
<div class="line">Note that both gprof and pfmon or any performance timer will slow down code.  Remove -pg from makefile to avoid slowdown as well.</div>
<div class="line"></div>
<div class="line">If not using the SIMD (single instruction, multiple data), or SSE type operations (which automatically are tried if using most compilers with optimizations), then roughly:</div>
<div class="line">CPU</div>
<div class="line">cycles | operation or procedure</div>
<div class="line">---------------------------------------------</div>
<div class="line">1          addition, subtraction, comparison</div>
<div class="line">2          fabs</div>
<div class="line">3          <a class="code" href="../../dc/dc2/f2c_8h.html#a3aa069ac3980707dae1e0530f50d59e4">abs</a></div>
<div class="line">4          multiplication</div>
<div class="line">10         division, modulus</div>
<div class="line">20         sqrt</div>
<div class="line">50         exp</div>
<div class="line">60         sin, cos, tan</div>
<div class="line">80         asin, acos, atan</div>
<div class="line">100        pow</div>
<div class="line">10         Miss L1 cache</div>
<div class="line">50         Miss L2 cache if L3 cache present</div>
<div class="line">150        Branch misprediction</div>
<div class="line">200        Miss L3 cache or miss L2 cache if no L3 cache present</div>
<div class="line">200-1000+  Page fault</div>
<div class="line"></div>
<div class="line">Also, note that there are more than just math operations to worry about.  Another rule of thumb is that:</div>
<div class="line"></div>
<div class="line">page fault &gt;&gt;&gt;&gt;&gt;&gt; cache miss &gt;&gt; branch mistaken &gt;&gt; dependency chain &gt;&gt; non-sse-sqrt, division, and modulus &gt; sse-sqrt etc. &gt; everything else</div>
<div class="line"></div>
<div class="line">with &quot;&gt;&gt;&quot; being much greater than (as in 3-20 times).</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">To get size of code&#39;s array elements:</div>
<div class="line"></div>
<div class="line">Use program &quot;nm&quot; to list objects and symbols, or use objdump -axfhp</div>
<div class="line">Use &quot;nm -S --size-sort&quot; to list by memory size</div>
<div class="line"></div>
<div class="line">To check global symbol sizes use:</div>
<div class="line">nm -S --size-sort grmhd </div>
<div class="line">or:  objdump -axfhp grmhd</div>
<div class="line"></div>
<div class="line">e.g. to check total size used do:</div>
<div class="line">1) nm -S --size-sort grmhd | awk &#39;{print $2}<span class="stringliteral">&#39; &gt; list.grmhd.sizes.txt</span></div>
<div class="line"><span class="stringliteral">2) total=0</span></div>
<div class="line"><span class="stringliteral">3) for fil in `cat list.grmhd.sizes.txt` ; do total=$(($total+0x$fil)) ; done</span></div>
<div class="line"><span class="stringliteral">4) echo $total</span></div>
<div class="line"><span class="stringliteral">Now take that number and divide by N1M*N2M*N3M and that&#39;</span>s approximate per zone bytes used  (must include boundary cells)</div>
<div class="line"></div>
<div class="line">Seems to be about 1250 elements/zone</div>
<div class="line"></div>
<div class="line">Checking FLOPS with papi</div>
<div class="line">http:<span class="comment">//www.cisl.ucar.edu/css/staff/rory/papi/flops.html</span></div>
<div class="line"></div>
<div class="line">with perfmon2 (pfmon):</div>
<div class="line">pfmon --show-time -e FP_COMP_OPS_EXE ./grmhd.makedir.testpfmon_simple 1 1 1</div>
<div class="line">Note that X87_OPS_RETIRED:any doesn<span class="stringliteral">&#39;t seem to make sense</span></div>
<div class="line"><span class="stringliteral">Then take resulting number and divide by the number of seconds for running the code WITHOUT pfmon!  That&#39;</span>s FLOPS.</div>
<div class="line">I get about 580MFLOPS  per core \sim 0.6GFLOPS per core</div>
<div class="line">Theoretical maximum is about 5.4GFLOPS, so I<span class="stringliteral">&#39;m about 10% efficient with FPU</span></div>
<div class="line"><span class="stringliteral">This shows that I&#39;</span>m memory limited</div>
<div class="line"></div>
<div class="line">When using new code with macrofied arrays and <a class="code" href="../../d8/d5b/defs_8general_8h.html#ae202d8b69a0ef3f07651368c3de9ca25">TIMEORDER</a>==2 and RK2 but still with <a class="code" href="../../de/d10/global_8nondepmnemonics_8h.html#a888099cf1c2f69f87cbd878ce5983af2">PARALINE</a>, I get 0.7GFLOPS since more focused on inversion than memory accesses.</div>
</div><!-- fragment --> 
<p>Definition in file <a class="el" href="../../da/dc9/optimizations_8txt_source.html">optimizations.txt</a>.</p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Fri May 20 2016 15:52:36 for HARM by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="../../doxygen.png" alt="doxygen"/>
</a> 1.8.3.1
</small></address>
</body>
</html>
